# (WeRateDogs tweet-data analysis)


## Data Wrangling Report
        
### INTRODUCTION

This project has it theme on data wrangling. Data wrangling involves three stages; data gathering, assessment and cleaning. In the data gathering stage, after a research requestion has been determined, potential data sources are identified from which relevant data would be obtained for analysis in other to answer the research questions. Possible sources of data include in-house database, external databases, data on web pages, etc. Data can be obtained from this data sources in myriad of ways which include querying a database, accessing data remotely via an API, programmatically downloading a dataset, scrapping data off a web page, also data can sometimes be readily available by hand for analysis. Once the required data has been gathered, the next stage of the wrangling process is the assessment stage. In this stage the gathered data is assessed for content issues (quality) and structural issues (tidiness). Raw uncleaned data can be very difficult to work with, hence the data is assessed for all possible issues ranging from missing data points, mispelled words, invalid data, inconsistency in the data etc., which all fall under data quality issues. Structural issues in the dataset to be investigated include; appropriate and informative column labels, column datatypes, units in the case of columns holding measurement values and also ensuring that the dataset meets the tidy data definition which is; a row is an observation, a column is an attribute and one observational unit forms a table. Once all possible data tidiness and quality issues have been identified, the next stage of the wrangling process is the cleaning stage. In this stage all assessments from the previous stage are trandformed into well defined cleaning operations which are then implemented with codes. After the codes have been written to correct assessed issues, the codes are tested to ensure they are producing desired results after which analysis can be carried out on the dataset or a further reassessment and cleaning could be done if new issues are identified in the process.
    
### DATA GATHERING AND ASSESSMENT
    
In this project, three data sets were used. The first dataset twitter_archive_enhanced was provided at hand, the image_prediction dataset was downloaded programmatically from the Udacity server via the provided url using the python request module, and the third dataset tweet_json was downloaded from the Udacity server as well. This data was supposed to be obtained via the twitter API, however, I couldn't get my application for a twitter developer account approved, hence I went with the option of using the one made available by Udacity by manually downloading it. After downloading the tweet_json file, the json strings were read and deserialized using the json module and the tweet_ids, retweet_count and favorite_count were extracted to a dictionary with keys; tweet_id, retweet_count and favorite_count, and appended to a list which is then passed to a pandas dataframe construct to return a dataframe object. After importing the datasets (df_twitter_archive_enhanced, df_image_prediction and df_tweet_json), each dataset was visually inspected for quality issues. For df_twitter_archive_enhanced, data quality issues assessed include; invalid names in the name column, inconsistency in the rating_denominator column, multiple entries and and non-twitter urls in the expanded_urls column. Also, the dataset was also found to contain retweets data. The data tidiness issues observed include; timestamp column having a object datatype designation. The df_image_prediction dataset was assessed of quality issues as; inconsistency in prediction label spellings of columns p1, p2 and p3. Some labels began with lowercase character while others had uppercase characters. Also columns p1_conf, p2_conf and p3_conf columns had excessively high precision digits, and it was also observed that some column labels were not very informative.
    
### DATA CLEANING AND STORAGE
    
After the datasets have been assessed the next stage in the data wrangling process is the cleaning stage. First, copies of the datasets were made, and the two tidiness issues were cleaned. The non discriptive label columns of the df_image_prediction dataset was resolved by renaming approriate column labels with more informative headers. In this cleaning, columns jpg_url, img_num, p1, p2, p3, p1_conf, p2_conf, p3_conf, p1_dog, p2_dog and p3_dog were renamed to image_url, image_number, prediction_1, prediction_2, prediction_3, p1_confidence, p2_confidence, p3_confidence, p1_is_dog, p2_is_dog and p3_is_dog respectively, via the pandas rename() method. The errorneous datatype of the timestamp column of the df_twitter_archive_enhanced dataframe was changed to datetime type using the pandas pd.to_datetime() method. After cleaning the data tidiness issues, the quality issues were addressed. For the df_image_prediction dataframe, columns p1, p2 and p3 entries were converted to title case strings using the title() string method. And then the p1_conf, p2_conf and p3_conf column values were rounded to 3 digit precision. In df_twitter_archive_enhanced, the expanded_urls column rows with more than one entry were splitted and reassigned to a single value which has to be a twitter url (some of the rows contained non-twitter urls). While rows with strictly non-twitter urls were dropped from the dataset via the row indexes. The rating_denominator column values that were different from 10 were replaced with 10. These values were few, and they could have been outliers. Next, the dataset was filtered for retweets as only original tweets were required for the analysis. This was done by filtering the dataset by null retweeted_status_id values after which the column was dropped alongside retweeted_status_user_id, retweeted_status_timestamp, in_reply_to_status_id and in_reply_to_user_id columns. In the name column of the dataset, invalid names 'a' and 'one' were observed, these might both have been spelling errors and since there is no idea what the correct names should actually be, the rows with these names were dropped. At the end of the data cleaning process, where all assessed quality and tidiness issues have been fixed, the dataframes df_image_prediction_clean and df_tweet_json_clean were merged on the tweet_id column of df_twitter_archive_enhanced_clean dataframe and saved as a csv file named twitter_archive_master. 

## Research Questions

1. Do high rated dog images have high retweet_count?
2. What dog stage image has the highest retweet_count?
3. Which of the neural network predictions is often of higher confidence of the three predictions of dog images?
4. Is there a relationship between the retweet_count and favorite_count columns?

## Summary of Findings

1. High rated dog images do not seem to have a strong positive correlation with high retweet_count. The regression plot line of best fit, however, suggest a rather weak correlation between these variables. 
2. Pupper dog stage images appears to have the highest retweet count followed by the doggo, with the puppo dog stage images having nearly twice the number of retweets as the floofer dog stage images. This suggests that people engage more with the pupper dog stage images, with less engagement with floofer dog stage images.
3. First image predictions of the neural network often tend to be of higher confidence than the second and third predictions respectively. This trend informs why the dorminating image number is 1. The image number 3 is scarcely found in the image number column as the third prediction often have the least confidence value. The reason for this observation could be further investigated.
4. There is a positive correlation of retweet count and favorite count. These variables appear to be moving in similar direction with strong correlation coefficient value of .91. This might be indicative of some form of relationship  between the two variables. 
